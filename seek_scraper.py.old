"""
Seek.co.nz Job Scraper
Extracts job listings from Seek New Zealand
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import csv
from datetime import datetime
from urllib.parse import urlencode, urlparse, parse_qs
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class SeekScraper:
    def __init__(self):
        self.session = requests.Session()
        self.base_url = "https://www.seek.co.nz"
        self.search_url = f"{self.base_url}/jobs"

        # Set headers to mimic a real browser
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
        }
        self.session.headers.update(self.headers)

        self.jobs_data = []

    def get_page(self, url, retry_count=3):
        """Fetch a page with retry logic"""
        for attempt in range(retry_count):
            try:
                logger.info(f"Fetching: {url}")
                response = self.session.get(url, timeout=10)

                if response.status_code == 200:
                    return response
                elif response.status_code == 403:
                    logger.warning("Access denied (403). Waiting before retry...")
                    time.sleep(5 * (attempt + 1))  # Exponential backoff
                else:
                    logger.warning(f"Status code: {response.status_code}")

            except Exception as e:
                logger.error(f"Error fetching page: {e}")
                time.sleep(3)

        return None

    def parse_job_card(self, card):
        """Extract job details from a job card element"""
        job = {}

        try:
            # Try to find the job title
            title_elem = card.find('a', {'data-automation': 'jobTitle'})
            if not title_elem:
                title_elem = card.find('h3')
            if not title_elem:
                title_elem = card.find('a', href=True)

            if title_elem:
                job['title'] = title_elem.get_text(strip=True)
                job['url'] = self.base_url + title_elem.get('href', '') if title_elem.get('href') else ''

            # Extract company name
            company_elem = card.find('a', {'data-automation': 'jobCompany'})
            if not company_elem:
                company_elem = card.find('span', {'data-testid': 'job-card-advertiser'})
            if company_elem:
                job['company'] = company_elem.get_text(strip=True)

            # Extract location
            location_elem = card.find('a', {'data-automation': 'jobLocation'})
            if not location_elem:
                location_elem = card.find('span', {'data-testid': 'job-card-location'})
            if location_elem:
                job['location'] = location_elem.get_text(strip=True)

            # Extract salary if available
            salary_elem = card.find('span', {'data-automation': 'jobSalary'})
            if not salary_elem:
                salary_elem = card.find('span', {'data-testid': 'job-card-salary'})
            if salary_elem:
                job['salary'] = salary_elem.get_text(strip=True)
            else:
                job['salary'] = 'Not specified'

            # Extract job type (Full time, Part time, etc.)
            type_elem = card.find('span', {'data-automation': 'job-card-work-type'})
            if type_elem:
                job['job_type'] = type_elem.get_text(strip=True)

            # Extract listing date
            date_elem = card.find('span', {'data-automation': 'jobListingDate'})
            if not date_elem:
                date_elem = card.find('time')
            if date_elem:
                job['date_listed'] = date_elem.get_text(strip=True)

            # Extract job description snippet
            desc_elem = card.find('span', {'data-automation': 'jobShortDescription'})
            if desc_elem:
                job['description'] = desc_elem.get_text(strip=True)

            return job if job.get('title') else None

        except Exception as e:
            logger.error(f"Error parsing job card: {e}")
            return None

    def scrape_search_results(self, params=None, max_pages=5):
        """Scrape job listings from search results"""

        if params is None:
            params = {
                'daterange': '31',  # Last 31 days
                'sortmode': 'ListedDate'  # Sort by date listed
            }

        page = 1
        total_jobs = 0

        while page <= max_pages:
            params['page'] = page
            url = f"{self.search_url}?{urlencode(params)}"

            response = self.get_page(url)
            if not response:
                logger.error(f"Failed to fetch page {page}")
                break

            soup = BeautifulSoup(response.text, 'html.parser')

            # Find job cards - Seek uses different selectors
            job_cards = soup.find_all('article')
            if not job_cards:
                job_cards = soup.find_all('div', {'data-automation': 'normalJob'})
            if not job_cards:
                job_cards = soup.find_all('div', {'data-testid': 'job-card'})

            if not job_cards:
                logger.warning(f"No job cards found on page {page}")
                # Try to find if there's a different structure
                main_content = soup.find('div', {'data-automation': 'searchResults'})
                if main_content:
                    job_cards = main_content.find_all('article')

            logger.info(f"Found {len(job_cards)} job cards on page {page}")

            if len(job_cards) == 0:
                break

            for card in job_cards:
                job = self.parse_job_card(card)
                if job:
                    self.jobs_data.append(job)
                    total_jobs += 1

            logger.info(f"Page {page}: Extracted {len(job_cards)} jobs. Total: {total_jobs}")

            # Check for next page
            next_button = soup.find('a', {'data-automation': 'page-next'})
            if not next_button or 'disabled' in next_button.get('class', []):
                logger.info("No more pages available")
                break

            page += 1
            time.sleep(2)  # Be respectful with rate limiting

        return self.jobs_data

    def save_to_csv(self, filename=None):
        """Save scraped jobs to CSV file"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'seek_jobs_{timestamp}.csv'

        if not self.jobs_data:
            logger.warning("No data to save")
            return

        # Get all unique keys from jobs
        all_keys = set()
        for job in self.jobs_data:
            all_keys.update(job.keys())

        with open(filename, 'w', newline='', encoding='utf-8') as f:
            writer = csv.DictWriter(f, fieldnames=sorted(all_keys))
            writer.writeheader()
            writer.writerows(self.jobs_data)

        logger.info(f"Saved {len(self.jobs_data)} jobs to {filename}")
        return filename

    def save_to_json(self, filename=None):
        """Save scraped jobs to JSON file"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'seek_jobs_{timestamp}.json'

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.jobs_data, f, indent=2, ensure_ascii=False)

        logger.info(f"Saved {len(self.jobs_data)} jobs to {filename}")
        return filename

    def search_jobs(self, keywords=None, location=None, job_type=None, date_range=31, max_pages=5):
        """Search for jobs with specific criteria"""
        params = {
            'daterange': str(date_range),
            'sortmode': 'ListedDate'
        }

        if keywords:
            params['keywords'] = keywords

        if location:
            params['where'] = location

        if job_type:
            params['worktype'] = job_type

        logger.info(f"Searching with params: {params}")
        return self.scrape_search_results(params, max_pages)


def main():
    """Main function to demonstrate usage"""
    scraper = SeekScraper()

    # Example 1: Scrape recent jobs (default parameters)
    print("Scraping recent job listings from Seek.co.nz...")
    print("=" * 50)

    jobs = scraper.search_jobs(
        keywords=None,  # Set to specific keywords if needed, e.g., "python developer"
        location=None,  # Set to specific location if needed, e.g., "Auckland"
        date_range=31,  # Jobs from last 31 days
        max_pages=2     # Scrape first 2 pages
    )

    if jobs:
        print(f"\nSuccessfully scraped {len(jobs)} jobs!")
        print("\nSample of scraped jobs:")
        print("-" * 50)

        for i, job in enumerate(jobs[:5], 1):  # Show first 5 jobs
            print(f"\n{i}. {job.get('title', 'N/A')}")
            print(f"   Company: {job.get('company', 'N/A')}")
            print(f"   Location: {job.get('location', 'N/A')}")
            print(f"   Salary: {job.get('salary', 'Not specified')}")
            print(f"   Posted: {job.get('date_listed', 'N/A')}")
            if job.get('url'):
                print(f"   URL: {job['url']}")

        # Save to files
        csv_file = scraper.save_to_csv()
        json_file = scraper.save_to_json()

        print(f"\nData saved to:")
        print(f"  - CSV: {csv_file}")
        print(f"  - JSON: {json_file}")
    else:
        print("No jobs were scraped. The website structure might have changed.")
        print("Please check the logs for more details.")


if __name__ == "__main__":
    main()